
from fastapi import FastAPI, Request
import os
import requests
from telegram import Bot
from telegram.constants import ParseMode
from bs4 import BeautifulSoup
import re
from datetime import datetime, date, timedelta
import json

app = FastAPI()

TOKEN = os.getenv("TOKEN")
if not TOKEN:
    raise ValueError("TOKEN is not set!")

ADMINS = os.getenv("ADMINS", "")

READ_LIST_FROM_ENV = True if os.getenv("READ_LIST_FROM_ENV").lower() == "true" else False

user_preferences = {}  # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ØªÙ„ÙØ¸ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
user_pos = {}  # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª ØªÙ„ÙØ¸ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† (br/us)
USER_FILE = "users.json"

API_URL = f"https://api.telegram.org/bot{TOKEN}/sendMessage"

# ----------------- UTILITIES -----------------
def save_user(user_id):
    try:
        users = {}
        if os.path.exists(USER_FILE):
            with open(USER_FILE, "r") as f:
                users = json.load(f)
        else:
            with open(USER_FILE, "w") as f:
                json.dump({}, f)
        if str(user_id) not in ADMINS and str(user_id) not in users:
            users[str(user_id)] = datetime.now().strftime("%Y-%m-%d")
            with open(USER_FILE, "w") as f:
                json.dump(users, f)
            print("User saved successfully")
            print(">>>> users", users)
    except Exception as e:
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ú©Ø§Ø±Ø¨Ø±:", e)

def get_user_stats():
    try:
        if not os.path.exists(USER_FILE):
            return {"total": 1, "today": 1, "yesterday": 1}

        with open(USER_FILE, "r") as f:
            users = json.load(f)
        total = len(users)
        print(">>> users count:", total)
        today = date.today().isoformat()
        yesterday = (date.today() - timedelta(days=1)).isoformat()

        today_count = sum(1 for d in users.values() if d == today)
        yesterday_count = sum(1 for d in users.values() if d == yesterday)

        return {
            "total": total + 1,
            "today": today_count + 1,
            "yesterday": yesterday_count + 1
        }
    except Exception as e:
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø¢Ù…Ø§Ø±:", e)
        return {"total": 1, "today": 1, "yesterday": 1}
# ----------------- LONGMAN PARSER -----------------
# Ù„ÛŒØ³Øª ØªØ¨Ø¯ÛŒÙ„ Ø§Ù…Ù„Ø§ÛŒ Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒÛŒ Ø¨Ù‡ Ø¨Ø±ÛŒØªÛŒØ´ ÛŒØ§ Ø¨Ø±Ø¹Ú©Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù„Ø§Ù†Ú¯Ù…Ù† ÛŒØ§ Ø¢Ú©Ø³ÙÙˆØ±Ø¯
equivalnet_spelling_list = {
 'acknowledgement': 'acknowledgment',
 'aluminium': 'aluminum',
 'anemia': 'anaemia',
 'analog': 'analogue',
#  'analyse': 'analyze',
 'analyze': 'analyse',
 'apologise': 'apologize',
#  'apologize': 'apologise',
 'appall': 'appal',
 'appetiser': 'appetizer',
#  'appetizer': 'appetiser',
 'arbor': 'arbour',
 'ardor': 'ardour',
 'armor': 'armour',
 'behavior': 'behaviour',
#  'behaviour': 'behavior',
 'cecum': 'caecum', # oxford
 'caecum': 'cecum',# oxford
 'caeca': 'caecum',# oxford - plural
 'caecum': 'caecum',# oxford - plural
 'cesium': 'caesium',
 'Cs': 'cesium',
 'caliber': 'calibre',
#  'calibre': 'caliber',
 'candor': 'candour',
 'catalog': 'catalogue',
 'catalyze': 'catalyse',
 'categorise': 'categorize',
 'cecum': 'caecum',
 'center': 'centre',
#  'centre': 'center',
 'cesium': 'caesium',
 'characterisation': 'characterization',
 'civilise': 'civilize',
#  'civilize': 'civilise',
 'clamor': 'clamour',
 'celiac': 'coeliac',
 'coeliac': 'celiac disease',
 'colonise': 'colonize',
 'colonize': 'colonise',
 'color': 'colour',
 'counseling': 'counselling',
#  'counselling': 'counseling',
#  'counsellor': 'counselor',
 'counselor': 'counsellor',
 'criticise': 'criticize',
#  'criticize': 'criticise',
 'defence': 'defense', # noun in UK is 'defence', verb is 'defense'
 'defense': 'defence',
 'demeanor': 'demeanour',
 'emphasise': 'emphasize',
#  'emphasize': 'emphasise',
 'encyclopaedia': 'encyclopedia',
#  'encyclopedia': 'encyclopaedia',
 'endeavor': 'endeavour',
#  'endeavour': 'endeavor',
 'enrollment': 'enrolment',
#  'enrolment': 'enrollment',
 'equaling': 'equal',
 'equalling': 'equal',
 'estrogen': 'oestrogen',
 'favorite': 'favourite',
 'favour': 'favor',
 'favourite': 'favorite',
 'fervor': 'fervour',
 'fiber': 'fibre',
 'fibre': 'fiber',
 'flavor': 'flavour',
#  'flavour': 'flavor',
 'fetus': 'foetus',
 'fueled': 'fuel',
 'fuelled': 'fuel',
 'fueling': 'fuelling',
 'fuelled': 'fueled',
 'fueling': 'fuel',
 'fuelling': 'fuel',
 'fulfill': 'fulfil',
#  'fulfils': 'fulfills',
 'glamor': 'glamour',
#  'glamour': 'glamor',
 'goiter': 'goitre',
 'harbor': 'harbour',
#  'honor': 'honour',
 'honour': 'honor',
#  'humor': 'humour',
 'humour': 'humor',
 'installment': 'instalment',
#  'instalment': 'installment',
 'instill': 'instil',
 'jewelry': 'jewellery',
#  'jewelry': 'jewellery',
 'kilometer': 'kilometre',
#  'kilometre': 'kilometer',
 'labor': 'labour',
#  'labour': 'labor',
 'leukaemia': 'leukemia',
#  'leukemia': 'leukaemia',
 'licence': 'license', # noun in UK is 'licence', verb is 'license' or 'licence'
 'license': 'licence',
 'liter': 'litre',
#  'litre': 'liter',
 'louver': 'louvre',
#  'louvre': 'louver',
 'luster': 'lustre',
#  'lustre': 'luster',
 'maneuver': 'manoeuvre',
#  'manoeuvre': 'maneuver',
 'marvelous': 'marvellous',
 'meager': 'meagre',
#  'meagre': 'meager',
 'mediaeval': 'medieval',
#  'medieval': 'mediaeval',
 'memorise': 'memorize',
#  'memorize': 'memorise',
 'meter': 'metre',
 'metre': 'meter',
 'minimise': 'minimize',
#  'minimize': 'minimise',
 'modeling': 'modelling',
 'mold': 'mould',
 'molt': 'moult',
 'mustache': 'moustache',
 'neighbor': 'neighbour',
#  'neighbour': 'neighbor',
 'odor': 'odour',
#  'odour': 'odor',
 'esophagus': 'oesophagus',
 'oestrogen': 'estrogen',
#  'offence': 'offense',
 'offense': 'offence',
 'organise': 'organize',
#  'organize': 'organise',
 'organisation': 'organzsation',
 'pediatric': 'paediatric',
 'pediatrician': 'paediatrician',
 'pedophile': 'paedophile',
#  'paralyse': 'paralyze',
 'paralyze': 'paralyse',
 'parlor': 'parlour',
 'patronise': 'patronize',
#  'patronize': 'patronise',
 'plow': 'plough',
 'practice': 'practise',   # noun in US/UK: practice; verb in UK: practise
 'practise': 'practice',
#  'pretence': 'pretense',
 'pretense': 'pretence',
 'prise': 'prize',
#  'prize': 'prise',
 'pajamas': 'pyjamas',
 'quarreling': 'quarrel',
 'quarrelling': 'quarrel',
 'rancor': 'rancour',
 'realise': 'realize',
#  'realize': 'realise',
 'recognise': 'recognize',
#  'recognize': 'recognise',
 'reveled': 'revel',
 'revelled': 'revel',
 'revelling': 'reveling',
 'rigor': 'rigour',
 'rumor': 'rumour',
#  'rumour': 'rumor',
 'saber': 'sabre',
#  'sabre': 'saber',
 'savior': 'saviour',
 'savor': 'savour',
#  'savour': 'savor',
 'scepter': 'sceptre',
#  'sceptre': 'scepter',
 'signaling': 'signal',
 'signalling': 'signal',
#  'skilful': 'skillful',
 'skillful': 'skilful',
 'smolder': 'smoulder',
 'socialise': 'socialize',
#  'socialize': 'socialise',
 'somber': 'sombre',
#  'sombre': 'somber',
 'specialise': 'specialize',
#  'specialize': 'specialise',
 'specter': 'spectre',
#  'spectre': 'specter',
 'splendor': 'splendour',
#  'splendour': 'splendor',
 'succor': 'succour',
 'tarif': 'tariff',
 'theater': 'theatre',
#  'theatre': 'theater',
 'traveled': 'travel',
 'travelled': 'travel',
 'traveler': 'traveller',
 'traveling': 'travelling',
#  'travelle': 'traveler',
#  'travelled': 'traveled',
#  'traveller': 'traveler',
#  'travelling': 'traveling',
 'tumor': 'tumour',
 'valor': 'valour',
 'vapor': 'vapour',
 'vigor': 'vigour',
#  'vigour': 'vigor',
 'visualisation': 'visualization',
 'whiskey': 'whisky',
 'whiskey': 'whiskies',
 'whiskey': 'whiskeys',
 'willful': 'wilful',
 'women': 'woman',
 }
BIO_SPELLING = os.getenv("Equivalnet_Spelling_List")
if READ_LIST_FROM_ENV and BIO_SPELLING:
    with open(BIO_SPELLING, "r") as f:
        equivalnet_spelling_list = json.load(f)

# Irregular plural forms
irregular_plural_list = {
  "children": "child",
  "feet": "foot",
  "geese": "goose",
  "men": "man",
  "women": "woman",
  "teeth": "tooth",
  "mice": "mouse",
  "lice": "louse",
  "oxen": "ox",
  "people": "person",
  "dice": "die",
  "cacti": "cactus",
  "fungi": "fungus",
  "nuclei": "nucleus",
  "syllabi": "syllabus",
  "alumni": "alumnus",
  "antennae": "antenna",
  "formulae": "formula",
  "bacteria": "bacterium",
  "media": "medium",
  "data": "datum",
  "criteria": "criterion",
  "phenomena": "phenomenon",
  "theses": "thesis",
  "analyses": "analysis",
  "diagnoses": "diagnosis",
  "parentheses": "parenthesis",
  "prognoses": "prognosis",
  "synopses": "synopsis",
  "axes": "axis",
  "matrices": "matrix",
  "vertices": "vertex",
  "indices": "index",
  "appendices": "appendix",
  "beaux": "beau",
  "chateaux": "chateau",
  "tableaux": "tableau",
  "lives": "life",
  "knives": "knife",
  "wives": "wife",
  "leaves": "leaf",
  "wolves": "wolf",
  "selves": "self",
  "elves": "elf",
  "loaves": "loaf",
  "halves": "half",
  "scarves": "scarf",
  "calves": "calf",
  "hooves": "hoof"
}

BIO_SPELLING = os.getenv("Irregular_Plural_List")
if READ_LIST_FROM_ENV and BIO_SPELLING:
    with open(BIO_SPELLING, "r") as f:
        irregular_plural_list = json.load(f)
        
# Try to reduce to possible base forms
def get_base_forms(word):
    base_forms = set()
    base_forms.add(word)

    # Irregulars
    if word in irregular_plural_list:
        base_forms.add(irregular_plural_list[word])

    # Regular patterns
    if word.endswith('ies'):
        base_forms.add(re.sub('ies$', 'y', word))
    if word.endswith('es'):
        base_forms.add(re.sub('es$', '', word))
    if word.endswith('s') and not word.endswith('ss'):
        base_forms.add(re.sub('s$', '', word))
    if word.endswith('ing'):
        base_forms.add(re.sub('ing$', '', word))
        base_forms.add(re.sub('ing$', 'e', word))
    if word.endswith('ed'):
        base_forms.add(re.sub('ed$', '', word))
        base_forms.add(re.sub('ed$', 'e', word))

    return list(base_forms)

# Main function
def find_word(word):
    if check_in_dictionary(word):
        return word  # Found original word

    for base in get_base_forms(word):
        if base != word and check_in_dictionary(base):
            return base  # Found base form

    return None  # Not found

def build_longman_link(word):
    return f"https://www.ldoceonline.com/dictionary/{word.lower().replace(' ', '-')}"
BIO_SPELLING = os.getenv("Irregular_Plural_List")
if READ_LIST_FROM_ENV and BIO_SPELLING:
    with open(BIO_SPELLING, "r") as f:
        irregular_plural_list = json.load(f)
# ----------------- OXFORD PARSER -----------------
def build_oxford_link(word):
    return f"https://www.oxfordlearnersdictionaries.com/definition/english/{word.lower().replace(' ', '-')}"

# ----------------- Process Word and Fetch dictionaries info -----------------
def has_invalid_parent_class(element):
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù„Ø¯â€ŒÙ‡Ø§ÛŒ Ø¹Ù†ØµØ± ØªØ§ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø±ÛŒØ´Ù‡
    parents = element.find_parents()
    for parent in parents:
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø´Ø§Ù…Ù„ Tail ÛŒØ§ DERIV Ù‡Ø³ØªÙ†Ø¯
        classes = parent.get('class', [])
        if isinstance(classes, list) and any(cls in classes for cls in ['Tail', 'DERIV']):
            return True
    return False

def get_audio_url(audio_url, preferred, pos, word, chat_id, caption):
    if audio_url:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(audio_url, headers=headers)
            if response.status_code == 200 and response.headers["Content-Type"].startswith("audio"):
                safe_word = re.sub(r'[^\w\-]+', '_', word)
                file_name = f"{safe_word}_{preferred}_{pos}.mp3"

                with open(file_name, "wb") as f:
                    f.write(response.content)

                with open(file_name, "rb") as audio_file:
                    files = {'audio': audio_file}
                    data = {'chat_id': chat_id, 'caption': caption}
                    send_audio_url = f"https://api.telegram.org/bot{TOKEN}/sendAudio"
                    res = requests.post(send_audio_url, data=data, files=files)
                    print("ğŸ“¤ Ø§Ø±Ø³Ø§Ù„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ:", res.json())

                os.remove(file_name)

        except Exception as e:
            error_reply = {
                "chat_id": chat_id,
                "text": f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ: {e}"
            } 
            res = requests.post(API_URL, json=error_reply)
            print("ğŸ“¤ Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø®Ø·Ø§:", res.json())
    else:
        fetch_oxford_audio_enabled = True
        reply = {
            "chat_id": chat_id,
            "text": f"âŒØªÙ„ÙØ¸ ØµÙˆØªÛŒ Ú©Ù„Ù…Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯"
        }
        res = requests.post(API_URL, json=reply)
        print("ğŸ“¤ØªÙ„ÙØ¸ ØµÙˆØªÛŒ Ú©Ù„Ù…Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯", res.json())
  
def fetch_longman_data(word):
    url = build_longman_link(word)
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.text, "html.parser")
        data = []

        entries = soup.find_all("span", class_="ldoceEntry Entry")
        
        for entry in entries:
            headword_tag = entry.find("span", class_="HWD")
            print(">>> entry:", entry)
            print(">>> headword_tag:", headword_tag)
            
            if not headword_tag:
                continue
        
            headword = headword_tag.get_text(strip=True).lower()
            if headword != word.lower() and headword != equivalnet_spelling_list.get(word, word):
                continue  # ÙÙ‚Ø· Ù…Ø¯Ø®Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ù‚ÛŒÙ‚Ø§ Ø®ÙˆØ¯ Ú©Ù„Ù…Ù‡ Ù‡Ø³ØªÙ†Ø¯
            
            pos_tag = entry.find("span", class_="POS")
            phonetic_tag = entry.find("span", class_="PRON")
            print(">>> phonetic_tag:", phonetic_tag)
            speakers = entry.find_all("span", class_="speaker")
            print(">>> speakers:", speakers)

            if (not pos_tag and word not in irregular_plural_list) or not speakers:
                continue
            
            isPhoneticValid = True
            if phonetic_tag is not None:
                isPhoneticValid = has_invalid_parent_class(phonetic_tag)
                if(isPhoneticValid):
                    phonetic_tag = None 

            pos = pos_tag.get_text(strip=True)
            phonetic = phonetic_tag.get_text(strip=True) if phonetic_tag else None
            
            british_audio = None
            american_audio = None
            print(">>> get data-src-mp3:")

            for spk in speakers:
                mp3_url = spk.get("data-src-mp3", "")
                print(">>> mp3_url:", mp3_url)

                if "breProns" in mp3_url and not british_audio:
                    british_audio = mp3_url
                elif "ameProns" in mp3_url and not american_audio:
                    american_audio = mp3_url
            # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ú©Ø¯Ø§Ù… Ø§Ø² ØµØ¯Ø§Ù‡Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù‡     
            if british_audio or american_audio:
                data.append({
                    "pos": pos,
                    "phonetic": phonetic,
                    "british": british_audio,
                    "american": american_audio
                })

        return data

    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ§Ú©Ø´ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ù†Ú¯Ù…Ù†: {e}")
        return []

def fetch_oxford_audio(word, preferred_accent):
    url = build_oxford_link(word)
    headers = {"User-Agent": "Mozilla/5.0"}
    data = {
            "audio_url": None,
            "phonetic": None,
            "pos": None
        }  
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f"âŒ Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ Ø¢Ú©Ø³ÙÙˆØ±Ø¯ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯. Status: {response.status_code}")
            return data

        soup = BeautifulSoup(response.text, "html.parser")
        
           # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© mp3
        accent_class = 'pron-us' if preferred_accent == 'american' else 'pron-uk'

        audio_div = soup.find('div', class_=lambda c: c and
                         'sound' in c.split() and
                         'audio_play_button' in c.split() and
                         accent_class in c.split())
        accent_class_name = 'phons_n_am' if preferred_accent == 'american' else 'prons_br'     

        accent_div = soup.find("div", class_=accent_class_name)

        # Ø§Ú¯Ø± div Ù¾ÛŒØ¯Ø§ Ø¨Ø´Ù‡ØŒ Ø¨Ø±Ø±Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ data-src-mp3
        if audio_div and audio_div.has_attr('data-src-mp3'):
            audio_url = audio_div['data-src-mp3']
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ ÙÙˆÙ†ØªÛŒÚ© Ù‡Ù… Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ù‡
            phonetic_tag = accent_div.find("span", class_="phon")
            phonetic = phonetic_tag.get_text(strip=True) if phonetic_tag else None
            pos_tag = soup.find("span", class_="pos")
            pos = pos_tag.get_text(strip=True) if pos_tag else None
        else:
            print("âŒ ØµØ¯Ø§ Ø¨Ø±Ø§ÛŒ Ù„Ù‡Ø¬Ù‡ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¢Ú©Ø³ÙÙˆØ±Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
            return data   

        # Ø¨Ø§Ø²Ú¯Ø´Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ POS Ùˆ ØªÙ„ÙØ¸ ØµÙˆØªÛŒ
        data = {
            "audio_url": audio_url,
            "phonetic": phonetic,
            "pos": pos
        }
        return data    

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ§Ú©Ø´ÛŒ ØªÙ„ÙØ¸ Ø¢Ú©Ø³ÙÙˆØ±Ø¯: {e}")
        return None

# ØªØºÛŒÛŒØ± Ø¯Ø± ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆÙˆÛŒØ³ Ø¢Ú©Ø³ÙÙˆØ±Ø¯ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù† ÙˆÙˆÛŒØ³ Ø¯Ø± Ù„Ø§Ù†Ú¯Ù…Ù†
async def process_word(chat_id, word):
    initial_word = word
    fetch_oxford_audio_enabled = False
    original_word = word
    parts_data = fetch_longman_data(word)

    # Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ„ÙØ¸ Ù†Ø¨ÙˆØ¯ Ùˆ Ù…Ø¹Ø§Ø¯Ù„ Ø¨Ø±ÛŒØªÛŒØ´ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªØŒ Ø§ÙˆÙ†Ùˆ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†
    if not parts_data and word in equivalnet_spelling_list:
        equivalnet_replaced = False
        alt_word = equivalnet_spelling_list[word]
        parts_data = fetch_longman_data(alt_word)
        if parts_data:
            word = alt_word  # Ø§Ú¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø¯Ø§Ø´ØªØŒ Ø§ÙˆÙ† Ú©Ù„Ù…Ù‡ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø´Ù‡
            equivalnet_replaced = True
    if READ_LIST_FROM_ENV and not equivalnet_replaced:
        print(">>> equivalnet_replaced:", equivalnet_replaced)
        changed_word_result = find_word(word)
        if changed_word_result:
            word = changed_word_result
            print(">>> changed_word_result:", changed_word_result)
              
    longman_link = build_longman_link(word)
    oxford_link = build_oxford_link(word)

    reply = {
        "chat_id": chat_id,
        "text": f"Ú©Ù„Ù…Ù‡: {original_word}\n\nğŸ“š Longman: {longman_link}\nğŸ“– Oxford: {oxford_link}"
    }
    res = requests.post(API_URL, json=reply)

    preferred = user_preferences.get(chat_id, "american")
    
    if len(parts_data) == 0: fetch_oxford_audio_enabled = True
    
    for entry in parts_data:
        pos = entry['pos']
        phonetic = entry['phonetic']
        audio_url = entry[preferred]

        caption = f"ğŸ”‰ {word} ({pos}) - longman"
        if phonetic:
            caption += f"\nğŸ“Œ /{phonetic}/ "
        get_audio_url(audio_url, preferred, pos, word, chat_id, caption)
    
    if(fetch_oxford_audio_enabled):  
        # Ø§Ú¯Ø± Ù‡ÛŒÚ† ÙˆÙˆÛŒØ³ÛŒ Ø¯Ø± Ù„Ø§Ù†Ú¯Ù…Ù† Ù†Ø¨ÙˆØ¯ØŒ ÙˆÙˆÛŒØ³ Ø¢Ú©Ø³ÙÙˆØ±Ø¯ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†
        oxford_data = fetch_oxford_audio(initial_word,preferred)
        if not oxford_data:
            print(">>> not oxford_data => oxford_data:", oxford_data)
            oxford_data = fetch_oxford_audio(word,preferred)
        if oxford_data:
            pos = oxford_data.get('pos') if oxford_data.get('pos') else ""
            phonetic = oxford_data.get('phonetic') if oxford_data.get('phonetic') else ""
            audio_url = oxford_data.get('audio_url') if oxford_data.get('audio_url') else ""
            caption = f"ğŸ”‰ {word} ({pos}) - oxford"
            if phonetic:
                caption += f"\nğŸ“Œ /{phonetic}/"
            get_audio_url(audio_url, preferred, pos, word, chat_id, caption)
                  
# ----------------- FastAPI Webhook -----------------
@app.post("/webhook/{token}")
async def webhook(token: str, request: Request):
    try:
        if token != TOKEN:
            return {"ok": False, "error": "Invalid token"}
        global ADMINS
        data = await request.json()
        # print("Received data:", data)
        if "message" in data:
            chat_id = data['message']['chat']['id']
            text = data['message'].get('text', '')
            user_id = data['message']['from']['id']

            if text == "/start":
                save_user(user_id)

              
                reply = {
                    "chat_id": chat_id,
                    "text": (
                        "Ø³Ù„Ø§Ù…! ğŸ‘‹ Ø¨Ù‡ Ø±Ø¨Ø§Øª ØªÙ„ÙØ¸ Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯.\n\n"
                        "ÛŒÚ© Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ù† Ø§Ø±Ø³Ø§Ù„ Ú©Ù† ØªØ§ Ù„ÛŒÙ†Ú©ØŒ ÙÙˆÙ†ØªÛŒÚ© Ùˆ ØªÙ„ÙØ¸ ØµÙˆØªÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ø±Ø§Øª Ø¨ÙØ±Ø³ØªÙ….\n\n"
                        "âœ… Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ØªÙ„ÙØ¸ ğŸ‡ºğŸ‡¸ American Ø§Ø³Øª.\n"
                        "Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ø§Ø±Ø³Ø§Ù„ /british ØªÙ„ÙØ¸ Ø±Ø§ Ø¨Ù‡ ğŸ‡¬ğŸ‡§ British ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¨Ø§ /american Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯."
                    )
                }
                res = requests.post(API_URL, json=reply)
                print("ğŸ“¤ Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø®ÙˆØ´â€ŒØ¢Ù…Ø¯:")

            elif text == "/british":
                user_preferences[chat_id] = "british"
                reply = {
                    "chat_id": chat_id,
                    "text": "âœ… ØªÙ„ÙØ¸ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø±ÙˆÛŒ ğŸ‡¬ğŸ‡§ British ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯!"
                }
                res = requests.post(API_URL, json=reply)
                print("ğŸ“¤ Ø§Ø±Ø³Ø§Ù„ ØªØºÛŒÛŒØ± ØªÙ†Ø¸ÛŒÙ… British:")

            elif text == "/american":
                user_preferences[chat_id] = "american"
                reply = {
                    "chat_id": chat_id,
                    "text": "âœ… ØªÙ„ÙØ¸ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø±ÙˆÛŒ ğŸ‡¬ğŸ‡§ American ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯!"
                }
                res = requests.post(API_URL, json=reply)
                print("ğŸ“¤ Ø¬ÙˆØ§Ø¨ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯:")
                
            elif text == "/stats":
                if str(user_id) not in ADMINS:
                    reply = {
                        "chat_id": chat_id,
                        "text": "â›”ï¸ ÙÙ‚Ø· Ø§Ø¯Ù…ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø² Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯."
                    }
                else:
                    stats = get_user_stats()
                    reply = {
                        "chat_id": chat_id,
                        "text": (
                            f"ğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù†:\n\n"
                            f"ğŸ‘¥ Ú©Ù„ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†: {stats['total']}\n"
                            f"ğŸŸ¢ Ø§Ù…Ø±ÙˆØ²: {stats['today']}\n"
                            f"ğŸ•’ Ø¯ÛŒØ±ÙˆØ²: {stats['yesterday']}"
                        )
                    }   
                res = requests.post(API_URL, json=reply)
                print("ğŸ“¤ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø¢Ù…Ø§Ø±:", res.json)
            else:
                save_user(user_id)
                await process_word(chat_id, text)
        return {"ok": True}
    except Exception as e:
        print("âŒ Ø®Ø·Ø§:", e)
        return {"ok": False, "error": str(e)}